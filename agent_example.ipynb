{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv( '.env')\n",
    "\n",
    "api_key = os.getenv(\"HF_TOKEN\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"HF_TOKEN environment variable is required\")\n",
    "\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if not openrouter_api_key:\n",
    "    raise ValueError(\"OPENROUTER_API_KEY environment variable is required\")\n",
    "\n",
    "\n",
    "openrouter_api_base = os.getenv(\"OPENROUTER_API_BASE\")\n",
    "if not openrouter_api_base:\n",
    "    raise ValueError(\"OPENROUTER_API_BASE environment variable is required\")\n",
    "\n",
    "\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "if not anthropic_api_key:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY environment variable is required\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zambra/git/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from smolagents import tool \n",
    "# import packages that are used in our tools\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "@tool\n",
    "def get_hugging_face_top_daily_paper() -> str:\n",
    "    \"\"\"\n",
    "    This is a tool that returns the most upvoted paper on Hugging Face daily papers.\n",
    "    It returns the title of the paper\n",
    "    \"\"\"\n",
    "    try:\n",
    "      url = \"<https://huggingface.co/papers>\"\n",
    "      response = requests.get(url)\n",
    "      response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    "      soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "      # Extract the title element from the JSON-like data in the \"data-props\" attribute\n",
    "      containers = soup.find_all('div', class_='SVELTE_HYDRATER contents')\n",
    "      top_paper = \"\"\n",
    "\n",
    "      for container in containers:\n",
    "          data_props = container.get('data-props', '')\n",
    "          if data_props:\n",
    "              try:\n",
    "                  # Parse the JSON-like string\n",
    "                  json_data = json.loads(data_props.replace('&quot;', '\"'))\n",
    "                  if 'dailyPapers' in json_data:\n",
    "                      top_paper = json_data['dailyPapers'][0]['title']\n",
    "              except json.JSONDecodeError:\n",
    "                  continue\n",
    "\n",
    "      return top_paper\n",
    "    except requests.exceptions.RequestException as e:\n",
    "      print(f\"Error occurred while fetching the HTML: {e}\")\n",
    "      return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "@tool\n",
    "def get_paper_id_by_title(title: str) -> str:\n",
    "    \"\"\"\n",
    "    This is a tool that returns the arxiv paper id by its title.\n",
    "    It returns the title of the paper\n",
    "\n",
    "    Args:\n",
    "        title: The paper title for which to get the id.\n",
    "    \"\"\"\n",
    "    api = HfApi()\n",
    "    papers = api.list_papers(query=title)\n",
    "    if papers:\n",
    "        paper = next(iter(papers))\n",
    "        return paper.id\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "@tool\n",
    "def download_paper_by_id(paper_id: str) -> None:\n",
    "    \"\"\"\n",
    "    This tool gets the id of a paper and downloads it from arxiv. It saves the paper locally \n",
    "    in the current directory as \"paper.pdf\".\n",
    "\n",
    "    Args:\n",
    "        paper_id: The id of the paper to download.\n",
    "    \"\"\"\n",
    "    paper = next(arxiv.Client().results(arxiv.Search(id_list=[paper_id])))\n",
    "    paper.download_pdf(filename=\"paper.pdf\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "@tool\n",
    "def read_pdf_file(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    This function reads the first three pages of a PDF file and returns its content as a string.\n",
    "    Args:\n",
    "        file_path: The path to the PDF file.\n",
    "    Returns:\n",
    "        A string containing the content of the PDF file.\n",
    "    \"\"\"\n",
    "    content = \"\"\n",
    "    reader = PdfReader('paper.pdf')\n",
    "    print(len(reader.pages))\n",
    "    pages = reader.pages[:3]\n",
    "    for page in pages:\n",
    "        content += page.extract_text()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from smolagents import LiteLLMModel\n",
    "\n",
    "# Configurar o modelo\n",
    "model = LiteLLMModel(model_id=\"claude-3-5-sonnet-20240620\",\n",
    "                     api_key=anthropic_api_key)\n",
    "\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Explique o conceito de gravidade\"}]}\n",
    "]\n",
    "\n",
    "# Ou com mais parâmetros\n",
    "response = model(\n",
    "    messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Summarize today's top paper on Hugging Face daily papers by reading it.</span>                                         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct ──────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mSummarize today's top paper on Hugging Face daily papers by reading it.\u001b[0m                                         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m HfApiModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">top_paper_title </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> get_hugging_face_top_daily_paper()</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Top paper title:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, top_paper_title)</span><span style=\"background-color: #272822\">                                                                     </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mget_hugging_face_top_daily_paper\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mTop paper title:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtop_paper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while fetching the HTML: No connection adapters were found for '<https://huggingface.co/papers>'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Top paper title: None\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Top paper title: None\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 0: Duration 6.43 seconds| Input tokens: 2,399 | Output tokens: 111]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 0: Duration 6.43 seconds| Input tokens: 2,399 | Output tokens: 111]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">search_results </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> web_search(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"top paper Hugging Face daily papers\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                       </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(search_results)</span><span style=\"background-color: #272822\">                                                                                          </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34msearch_results\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mweb_search\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtop paper Hugging Face daily papers\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                       \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msearch_results\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                          \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "## Search Results\n",
       "\n",
       "[Daily Papers - Hugging Face](https://huggingface.co/papers)\n",
       "Your daily dose of AI research from AK. Hugging Face. Models; Datasets; Spaces; Posts; Docs; Enterprise; Pricing \n",
       "Log In Sign Up new. Get trending papers in your email inbox once a day! Get trending papers in your email inbox! \n",
       "Subscribe. Daily Papers by AK and the research community. Search by arxiv id or title Submit a paper. Feb 7. \n",
       "Submitted ...\n",
       "\n",
       "[Exploring the Daily Papers Page on Hugging Face](https://huggingface.co/blog/daily-papers)\n",
       "In the fast-paced world of research, staying up-to-date with the latest advancements is crucial. To help developers\n",
       "and researchers keep a pulse on the cutting-edge of AI, Hugging Face introduced the Daily Papers page. Since its \n",
       "launch, Daily Papers has featured high-quality research selected by AK and researchers from the community. Over the\n",
       "past year, more than 3,700 papers have been ...\n",
       "\n",
       "[[N] Daily Papers by Hugging Face : r/MachineLearning - \n",
       "Reddit](https://www.reddit.com/r/MachineLearning/comments/13lh43m/n_daily_papers_by_hugging_face/)\n",
       "Hugging Face recently released this Daily Papers website inspired by Ahsen Khaliq's curated list of research papers\n",
       "from arXiv. According to Hugging Face's CTO, Julien Chaumond, \"AK has posted ~17,000 tweets daily, tirelessly \n",
       "curating the new research drops from Arxiv. This is our own \"AK feed\" directly on HF, where each paper is linked to\n",
       "its ...\n",
       "\n",
       "[HuggingFace Paper Explorer](https://huggingface-paper-explorer.vercel.app/)\n",
       "Discover top AI research papers from the HuggingFace community. ... All paper data and content belong to their \n",
       "respective owners and the HuggingFace community. Showing 61 unique papers. Baichuan-Omni Technical Report. \n",
       "Submitted by kenshinn. 69. 6. View on ...\n",
       "\n",
       "[Daily papers feed on Hugging Face - Bens \n",
       "Bites](https://news.bensbites.com/posts/1963-daily-papers-feed-on-hugging-face)\n",
       "Daily papers feed on Hugging Face huggingface.co. by bentossell 1 year ago ... Been trying to figure out the best \n",
       "way to follow papers, I was a AK patreon, now Substack subscriber, now I'll be following his Daily Papers. ... \n",
       "Paper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). ...\n",
       "\n",
       "[Awesome paper - Hugging Face Forums](https://discuss.huggingface.co/c/research/awesome-paper/8)\n",
       "This subcategory contains the awesome papers discussed by the Hugging Face team.\n",
       "\n",
       "[Exploring the Daily Papers Page on Hugging \n",
       "Face](https://github.com/huggingface/blog/diffs/2?base_sha=bc12231e3b9f2d728969dead7b8c34d7c40a9cd2&amp;head_user=adeen\n",
       "ayakup&amp;name=main&amp;pull_number=2353&amp;qualified_name=refs/heads/main&amp;sha1=bc12231e3b9f2d728969dead7b8c34d7c40a9cd2&amp;sha2\n",
       "=018490f86c87d0a2dc3bd20fe6c917ffd27887bb&amp;short_path=d55b744&amp;unchanged=expanded&amp;w=false)\n",
       "📑 Claim your Papers. On the Daily Papers page, you'll notice author names listed under the title of each paper. If\n",
       "you're one of the authors and have a Hugging Face account, you can claim your paper with a single click! After \n",
       "claiming, the paper will be automatically linked to your account, adding visibility to your research and helping \n",
       "...\n",
       "\n",
       "[blog/daily-papers.md at main · huggingface/blog · \n",
       "GitHub](https://github.com/huggingface/blog/blob/main/daily-papers.md)\n",
       "In the fast-paced world of research, staying up-to-date with the latest advancements is crucial. To help developers\n",
       "and researchers keep a pulse on the cutting-edge of AI, Hugging Face introduced the Daily Papers page. Since its \n",
       "launch, Daily Papers has featured high-quality research selected by AK and researchers from the community. Over the\n",
       "past year, more than 3,700 papers have been ...\n",
       "\n",
       "[Daily Papers - Hugging Face](https://huggingface.co/papers?search=true)\n",
       "Your daily dose of AI research from AK. Hugging Face. Models; Datasets; Spaces; Posts; Docs; Enterprise; Pricing \n",
       "Log In Sign Up new. Get trending papers in your email inbox once a day! Get trending papers in your email inbox! \n",
       "Subscribe. Daily Papers by AK and the research community. Search by arxiv id or title Submit a paper. Jan 24 ...\n",
       "\n",
       "[Daily AI Papers | All credits go to HuggingFace's Daily AI papers ...](https://gabrielchua.me/daily-ai-papers/)\n",
       "These summaries are automatically generated from HuggingFace's Daily Papers, using Gemini and GitHub actions. All \n",
       "credits go to the research community for sharing and the HuggingFace community for curating these papers. Please \n",
       "note: Authors may be listed by their HuggingFace user id. This will be rectified soon.\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "## Search Results\n",
       "\n",
       "[Daily Papers - Hugging Face](https://huggingface.co/papers)\n",
       "Your daily dose of AI research from AK. Hugging Face. Models; Datasets; Spaces; Posts; Docs; Enterprise; Pricing \n",
       "Log In Sign Up new. Get trending papers in your email inbox once a day! Get trending papers in your email inbox! \n",
       "Subscribe. Daily Papers by AK and the research community. Search by arxiv id or title Submit a paper. Feb 7. \n",
       "Submitted ...\n",
       "\n",
       "[Exploring the Daily Papers Page on Hugging Face](https://huggingface.co/blog/daily-papers)\n",
       "In the fast-paced world of research, staying up-to-date with the latest advancements is crucial. To help developers\n",
       "and researchers keep a pulse on the cutting-edge of AI, Hugging Face introduced the Daily Papers page. Since its \n",
       "launch, Daily Papers has featured high-quality research selected by AK and researchers from the community. Over the\n",
       "past year, more than 3,700 papers have been ...\n",
       "\n",
       "[[N] Daily Papers by Hugging Face : r/MachineLearning - \n",
       "Reddit](https://www.reddit.com/r/MachineLearning/comments/13lh43m/n_daily_papers_by_hugging_face/)\n",
       "Hugging Face recently released this Daily Papers website inspired by Ahsen Khaliq's curated list of research papers\n",
       "from arXiv. According to Hugging Face's CTO, Julien Chaumond, \"AK has posted ~17,000 tweets daily, tirelessly \n",
       "curating the new research drops from Arxiv. This is our own \"AK feed\" directly on HF, where each paper is linked to\n",
       "its ...\n",
       "\n",
       "[HuggingFace Paper Explorer](https://huggingface-paper-explorer.vercel.app/)\n",
       "Discover top AI research papers from the HuggingFace community. ... All paper data and content belong to their \n",
       "respective owners and the HuggingFace community. Showing 61 unique papers. Baichuan-Omni Technical Report. \n",
       "Submitted by kenshinn. 69. 6. View on ...\n",
       "\n",
       "[Daily papers feed on Hugging Face - Bens \n",
       "Bites](https://news.bensbites.com/posts/1963-daily-papers-feed-on-hugging-face)\n",
       "Daily papers feed on Hugging Face huggingface.co. by bentossell 1 year ago ... Been trying to figure out the best \n",
       "way to follow papers, I was a AK patreon, now Substack subscriber, now I'll be following his Daily Papers. ... \n",
       "Paper pages allow people to find artifacts related to a paper such as models, datasets and apps/demos (Spaces). ...\n",
       "\n",
       "[Awesome paper - Hugging Face Forums](https://discuss.huggingface.co/c/research/awesome-paper/8)\n",
       "This subcategory contains the awesome papers discussed by the Hugging Face team.\n",
       "\n",
       "[Exploring the Daily Papers Page on Hugging \n",
       "Face](https://github.com/huggingface/blog/diffs/2?base_sha=bc12231e3b9f2d728969dead7b8c34d7c40a9cd2&head_user=adeen\n",
       "ayakup&name=main&pull_number=2353&qualified_name=refs/heads/main&sha1=bc12231e3b9f2d728969dead7b8c34d7c40a9cd2&sha2\n",
       "=018490f86c87d0a2dc3bd20fe6c917ffd27887bb&short_path=d55b744&unchanged=expanded&w=false)\n",
       "📑 Claim your Papers. On the Daily Papers page, you'll notice author names listed under the title of each paper. If\n",
       "you're one of the authors and have a Hugging Face account, you can claim your paper with a single click! After \n",
       "claiming, the paper will be automatically linked to your account, adding visibility to your research and helping \n",
       "...\n",
       "\n",
       "[blog/daily-papers.md at main · huggingface/blog · \n",
       "GitHub](https://github.com/huggingface/blog/blob/main/daily-papers.md)\n",
       "In the fast-paced world of research, staying up-to-date with the latest advancements is crucial. To help developers\n",
       "and researchers keep a pulse on the cutting-edge of AI, Hugging Face introduced the Daily Papers page. Since its \n",
       "launch, Daily Papers has featured high-quality research selected by AK and researchers from the community. Over the\n",
       "past year, more than 3,700 papers have been ...\n",
       "\n",
       "[Daily Papers - Hugging Face](https://huggingface.co/papers?search=true)\n",
       "Your daily dose of AI research from AK. Hugging Face. Models; Datasets; Spaces; Posts; Docs; Enterprise; Pricing \n",
       "Log In Sign Up new. Get trending papers in your email inbox once a day! Get trending papers in your email inbox! \n",
       "Subscribe. Daily Papers by AK and the research community. Search by arxiv id or title Submit a paper. Jan 24 ...\n",
       "\n",
       "[Daily AI Papers | All credits go to HuggingFace's Daily AI papers ...](https://gabrielchua.me/daily-ai-papers/)\n",
       "These summaries are automatically generated from HuggingFace's Daily Papers, using Gemini and GitHub actions. All \n",
       "credits go to the research community for sharing and the HuggingFace community for curating these papers. Please \n",
       "note: Authors may be listed by their HuggingFace user id. This will be rectified soon.\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 7.25 seconds| Input tokens: 5,003 | Output tokens: 209]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 7.25 seconds| Input tokens: 5,003 | Output tokens: 209]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">daily_papers_page_content </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> visit_webpage(url</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"https://huggingface.co/papers\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                 </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(daily_papers_page_content)</span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mdaily_papers_page_content\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mvisit_webpage\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34murl\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mhttps://huggingface.co/papers\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                 \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdaily_papers_page_content\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Daily Papers - Hugging Face\n",
       "\n",
       "[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)\n",
       "Hugging Face](/)\n",
       "\n",
       "* [Models](/models)\n",
       "* [Datasets](/datasets)\n",
       "* [Spaces](/spaces)\n",
       "* [Posts](/posts)\n",
       "* [Docs](/docs)\n",
       "* [Enterprise](/enterprise)\n",
       "* [Pricing](/pricing)\n",
       "* ---\n",
       "* [Log In](/login)\n",
       "* [Sign Up](/join)\n",
       "\n",
       "new\n",
       "\n",
       "Get trending papers in your email inbox once a day!\n",
       "\n",
       "Get trending papers in your email inbox!\n",
       "\n",
       "[Subscribe](/login?next=%2Fpapers)\n",
       "\n",
       "[Daily Papers](/papers)\n",
       "=======================\n",
       "\n",
       "by [AK](/akhaliq) and the research community\n",
       "--------------------------------------------\n",
       "\n",
       "Search by arxiv id or title\n",
       "\n",
       "[Submit a paper](/spaces/huggingface/HuggingDiscussions/discussions/32)\n",
       "Feb\n",
       "14\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08946.png)](/papers/2502.08946)\n",
       "Submitted by\n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg)\n",
       "akhaliq\n",
       "[156](/login?next=%2Fpapers%2F2502.08946)\n",
       "\n",
       "### [The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept \n",
       "Understanding](/papers/2502.08946)\n",
       "\n",
       "[* \n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/63fe24448b3c5087ff866b39/CaTM4yAfj9tJj53ciQWJk.jpeg)\n",
       "* ![](/avatars/0d7dfbd681b1157a38d0f0a86f19b702.svg)\n",
       "* ![](/avatars/f9757030d82c69aef933309e0c83ccd0.svg)\n",
       "* ·\n",
       "  8 authors](/papers/2502.08946)\n",
       "[3](/papers/2502.08946#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08910.png)](/papers/2502.08910)\n",
       "Submitted by\n",
       "![](/avatars/4aae2aca70ea9dc58dd6f9f9b2be15e1.svg)\n",
       "geonp\n",
       "[117](/login?next=%2Fpapers%2F2502.08910)\n",
       "\n",
       "### [InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU](/papers/2502.08910)\n",
       "\n",
       "[* ![](/avatars/e180e063c810c15d02b494727e962b84.svg)\n",
       "* ![](/avatars/4aae2aca70ea9dc58dd6f9f9b2be15e1.svg)\n",
       "* ![](/avatars/8c47b5c862f82d4258ba707c932f7f87.svg)\n",
       "* ·\n",
       "  4 authors](/papers/2502.08910)\n",
       "[6](/papers/2502.08910#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08690.png)](/papers/2502.08690)\n",
       "Submitted by\n",
       "![](/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg)\n",
       "Agorium\n",
       "[32](/login?next=%2Fpapers%2F2502.08690)\n",
       "\n",
       "### [Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation](/papers/2502.08690)\n",
       "\n",
       "[* ![](/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg)\n",
       "* ·\n",
       "  4 authors](/papers/2502.08690)\n",
       "[2](/papers/2502.08690#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09619.png)](/papers/2502.09619)\n",
       "Submitted by\n",
       "![](/avatars/113f02c1b1f8d33d3487daa867afcd3f.svg)\n",
       "jonkahana\n",
       "[28](/login?next=%2Fpapers%2F2502.09619)\n",
       "\n",
       "### [Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights](/papers/2502.09619)\n",
       "\n",
       "[* ![](/avatars/19d6ab141ec2cd25c1c3b45fd8f69910.svg)\n",
       "* ![](https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg)\n",
       "* ![](/avatars/113f02c1b1f8d33d3487daa867afcd3f.svg)\n",
       "* ·\n",
       "  4 authors](/papers/2502.09619)\n",
       "[2](/papers/2502.09619#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09560.png)](/papers/2502.09560)\n",
       "Submitted by\n",
       "![](/avatars/9bb8205b889337df5d321539c9b5d69d.svg)\n",
       "Ray2333\n",
       "[27](/login?next=%2Fpapers%2F2502.09560)\n",
       "\n",
       "### [EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied \n",
       "Agents](/papers/2502.09560)\n",
       "\n",
       "[* ![](/avatars/d9d8cfb5f112e6ed7f6152fc230135d3.svg)\n",
       "* ![](/avatars/02f1a9e2ce333ff521d901cf83fcdff3.svg)\n",
       "* ![](/avatars/81556de3214c848b3c3e118f50fd2968.svg)\n",
       "* ![](/avatars/c8c5b2706644fb45a75f13af99fa7ae9.svg)\n",
       "* ![](/avatars/c8b9ec7c00773c5a4055ba50de0c6b2f.svg)\n",
       "* ·\n",
       "  13 authors](/papers/2502.09560)\n",
       "[2](/papers/2502.09560#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09056.png)](/papers/2502.09056)\n",
       "Submitted by\n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg)\n",
       "akhaliq\n",
       "[26](/login?next=%2Fpapers%2F2502.09056)\n",
       "\n",
       "### [An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model \n",
       "Merging](/papers/2502.09056)\n",
       "\n",
       "[* \n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/63f6a050b4c9a104f4b95755/eJQyJkenSz536j-EGcpkH.jpeg)\n",
       "* ![](https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg)\n",
       "* ![](https://cdn-avatars.huggingface.co/v1/production/uploads/62d192c2d50433c35eb1b48e/VjmDu8GOIuLuQNBQdQLLS.png)\n",
       "* ·\n",
       "  4 authors](/papers/2502.09056)\n",
       "[4](/papers/2502.09056#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09604.png)](/papers/2502.09604)\n",
       "Submitted by\n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/1650651305661-5df84571da6d0311fd3d5407.png)\n",
       "voidism\n",
       "[26](/login?next=%2Fpapers%2F2502.09604)\n",
       "\n",
       "### [SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models](/papers/2502.09604)\n",
       "\n",
       "[* ![](/avatars/87708c86c1baef548ef556f5d32dca71.svg)\n",
       "* ![](https://cdn-ava\n",
       "..._This content has been truncated to stay below 10000 characters_...\n",
       "-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg)\n",
       "* ·\n",
       "  4 authors](/papers/2502.09042)\n",
       "[2](/papers/2502.09042#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09390.png)](/papers/2502.09390)\n",
       "Submitted by\n",
       "![](/avatars/4619930d15512ec9b80b01c62e986217.svg)\n",
       "danf\n",
       "[12](/login?next=%2Fpapers%2F2502.09390)\n",
       "\n",
       "### [SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language \n",
       "Models](/papers/2502.09390)\n",
       "\n",
       "[* ![](/avatars/e05ca715004b39e79472399f75010bda.svg)\n",
       "* ![](/avatars/b0022ad6dd992c75a51973070b302315.svg)\n",
       "* ![](https://cdn-avatars.huggingface.co/v1/production/uploads/63e0c8875c6964861ebb0c49/yzkhPSxgXtJCM62iMBOOK.jpeg)\n",
       "* ![](/avatars/4619930d15512ec9b80b01c62e986217.svg)\n",
       "* ·\n",
       "  4 authors](/papers/2502.09390)\n",
       "[2](/papers/2502.09390#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09601.png)](/papers/2502.09601)\n",
       "Submitted by\n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg)\n",
       "horseee\n",
       "[11](/login?next=%2Fpapers%2F2502.09601)\n",
       "\n",
       "### [CoT-Valve: Length-Compressible Chain-of-Thought Tuning](/papers/2502.09601)\n",
       "\n",
       "[* ![](/avatars/752e9d86018e7d33ad8bcd741203fd86.svg)\n",
       "* ![](/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg)\n",
       "* ![](/avatars/9216bdd9ed10c226f2d14edce4a10daa.svg)\n",
       "* ![](https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg)\n",
       "* ·\n",
       "  5 authors](/papers/2502.09601)\n",
       "[2](/papers/2502.09601#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08468.png)](/papers/2502.08468)\n",
       "Submitted by\n",
       "![](/avatars/b72b18130664c1de197c1f8df371aa70.svg)\n",
       "Haon-Chen\n",
       "[10](/login?next=%2Fpapers%2F2502.08468)\n",
       "\n",
       "### [mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data](/papers/2502.08468)\n",
       "\n",
       "[* ![](/avatars/be97941e60064e5dd806c6fe9db3c537.svg)\n",
       "* ![](/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg)\n",
       "* ![](/avatars/788e339472999a9159f77f857817d618.svg)\n",
       "* ![](https://cdn-avatars.huggingface.co/v1/production/uploads/625e62452a7279d3c77b5c38/zJINew6U4_Gup4WTobb-0.jpeg)\n",
       "* ![](/avatars/b72b18130664c1de197c1f8df371aa70.svg)\n",
       "* ·\n",
       "  7 authors](/papers/2502.08468)\n",
       "[2](/papers/2502.08468#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08680.png)](/papers/2502.08680)\n",
       "Submitted by\n",
       "![](/avatars/6b67eba3f15d6cd86ac3ad55c1daf166.svg)\n",
       "guactastesgood\n",
       "[9](/login?next=%2Fpapers%2F2502.08680)\n",
       "\n",
       "### [Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical\n",
       "Ranges](/papers/2502.08680)\n",
       "\n",
       "[* ![](/avatars/6b67eba3f15d6cd86ac3ad55c1daf166.svg)\n",
       "* ![](/avatars/457eae5e56b9641ee5543146447d1755.svg)\n",
       "* ·\n",
       "  3 authors](/papers/2502.08680)\n",
       "[2](/papers/2502.08680#community)\n",
       "\n",
       "Submitted by\n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t4fI-3djMfgXCchU_xpjL.png)\n",
       "xymeow7\n",
       "[9](/login?next=%2Fpapers%2F2502.09614)\n",
       "\n",
       "### [DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human \n",
       "References](/papers/2502.09614)\n",
       "\n",
       "[* ![](https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t4fI-3djMfgXCchU_xpjL.png)\n",
       "* ·\n",
       "  5 authors](/papers/2502.09614)\n",
       "[2](/papers/2502.09614#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05761.png)](/papers/2502.05761)\n",
       "Submitted by\n",
       "![](/avatars/a4b7b7fd6c1fca0eac85da7383f58361.svg)\n",
       "enquan2022\n",
       "[6](/login?next=%2Fpapers%2F2502.05761)\n",
       "\n",
       "### [3CAD: A Large-Scale Real-World 3C Product Dataset for Unsupervised Anomaly](/papers/2502.05761)\n",
       "\n",
       "[* ![](/avatars/a4b7b7fd6c1fca0eac85da7383f58361.svg)\n",
       "* ·\n",
       "  7 authors](/papers/2502.05761)\n",
       "[2](/papers/2502.05761#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09613.png)](/papers/2502.09613)\n",
       "Submitted by\n",
       "![](/avatars/172678e8187acfec0aa8e647479cbb81.svg)\n",
       "chaoyizh\n",
       "[5](/login?next=%2Fpapers%2F2502.09613)\n",
       "\n",
       "### [Latent Radiance Fields with 3D-aware 2D Representations](/papers/2502.09613)\n",
       "\n",
       "[* ·\n",
       "  4 authors](/papers/2502.09613)\n",
       "[2](/papers/2502.09613#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05979.png)](/papers/2502.05979)\n",
       "Submitted by\n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg)\n",
       "BestWishYsh\n",
       "[4](/login?next=%2Fpapers%2F2502.05979)\n",
       "\n",
       "### [VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer](/papers/2502.05979)\n",
       "\n",
       "[* ![](/avatars/6cb41b80cc5e014e455dfc2a22682e64.svg)\n",
       "* ![](/avatars/06a3f3cd533caa459f3da1bb5e0f4c1f.svg)\n",
       "* ·\n",
       "  7 authors](/papers/2502.05979)\n",
       "[2](/papers/2502.05979#community)\n",
       "\n",
       "[Previous](/papers?date=2025-02-13)\n",
       "\n",
       "System theme\n",
       "\n",
       "Company\n",
       "[TOS](/terms-of-service)\n",
       "[Privacy](/privacy)\n",
       "[About](/huggingface)\n",
       "[Jobs](https://apply.workable.com/huggingface/)\n",
       "\n",
       "Website\n",
       "[Models](/models)\n",
       "[Datasets](/datasets)\n",
       "[Spaces](/spaces)\n",
       "[Pricing](/pricing)\n",
       "[Docs](/docs)\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Daily Papers - Hugging Face\n",
       "\n",
       "[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)\n",
       "Hugging Face](/)\n",
       "\n",
       "* [Models](/models)\n",
       "* [Datasets](/datasets)\n",
       "* [Spaces](/spaces)\n",
       "* [Posts](/posts)\n",
       "* [Docs](/docs)\n",
       "* [Enterprise](/enterprise)\n",
       "* [Pricing](/pricing)\n",
       "* ---\n",
       "* [Log In](/login)\n",
       "* [Sign Up](/join)\n",
       "\n",
       "new\n",
       "\n",
       "Get trending papers in your email inbox once a day!\n",
       "\n",
       "Get trending papers in your email inbox!\n",
       "\n",
       "[Subscribe](/login?next=%2Fpapers)\n",
       "\n",
       "[Daily Papers](/papers)\n",
       "=======================\n",
       "\n",
       "by [AK](/akhaliq) and the research community\n",
       "--------------------------------------------\n",
       "\n",
       "Search by arxiv id or title\n",
       "\n",
       "[Submit a paper](/spaces/huggingface/HuggingDiscussions/discussions/32)\n",
       "Feb\n",
       "14\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08946.png)](/papers/2502.08946)\n",
       "Submitted by\n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg)\n",
       "akhaliq\n",
       "[156](/login?next=%2Fpapers%2F2502.08946)\n",
       "\n",
       "### [The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept \n",
       "Understanding](/papers/2502.08946)\n",
       "\n",
       "[* \n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/63fe24448b3c5087ff866b39/CaTM4yAfj9tJj53ciQWJk.jpeg)\n",
       "* ![](/avatars/0d7dfbd681b1157a38d0f0a86f19b702.svg)\n",
       "* ![](/avatars/f9757030d82c69aef933309e0c83ccd0.svg)\n",
       "* ·\n",
       "  8 authors](/papers/2502.08946)\n",
       "[3](/papers/2502.08946#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08910.png)](/papers/2502.08910)\n",
       "Submitted by\n",
       "![](/avatars/4aae2aca70ea9dc58dd6f9f9b2be15e1.svg)\n",
       "geonp\n",
       "[117](/login?next=%2Fpapers%2F2502.08910)\n",
       "\n",
       "### [InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on a Single GPU](/papers/2502.08910)\n",
       "\n",
       "[* ![](/avatars/e180e063c810c15d02b494727e962b84.svg)\n",
       "* ![](/avatars/4aae2aca70ea9dc58dd6f9f9b2be15e1.svg)\n",
       "* ![](/avatars/8c47b5c862f82d4258ba707c932f7f87.svg)\n",
       "* ·\n",
       "  4 authors](/papers/2502.08910)\n",
       "[6](/papers/2502.08910#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08690.png)](/papers/2502.08690)\n",
       "Submitted by\n",
       "![](/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg)\n",
       "Agorium\n",
       "[32](/login?next=%2Fpapers%2F2502.08690)\n",
       "\n",
       "### [Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient Text-to-Image Generation](/papers/2502.08690)\n",
       "\n",
       "[* ![](/avatars/57b91a488ac1745b3c0509c04eb6ad93.svg)\n",
       "* ·\n",
       "  4 authors](/papers/2502.08690)\n",
       "[2](/papers/2502.08690#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09619.png)](/papers/2502.09619)\n",
       "Submitted by\n",
       "![](/avatars/113f02c1b1f8d33d3487daa867afcd3f.svg)\n",
       "jonkahana\n",
       "[28](/login?next=%2Fpapers%2F2502.09619)\n",
       "\n",
       "### [Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights](/papers/2502.09619)\n",
       "\n",
       "[* ![](/avatars/19d6ab141ec2cd25c1c3b45fd8f69910.svg)\n",
       "* ![](https://cdn-avatars.huggingface.co/v1/production/uploads/630dd4218df86f1e5beb2ed7/fKvNWyWv6CVBdbXXUlrYv.jpeg)\n",
       "* ![](/avatars/113f02c1b1f8d33d3487daa867afcd3f.svg)\n",
       "* ·\n",
       "  4 authors](/papers/2502.09619)\n",
       "[2](/papers/2502.09619#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09560.png)](/papers/2502.09560)\n",
       "Submitted by\n",
       "![](/avatars/9bb8205b889337df5d321539c9b5d69d.svg)\n",
       "Ray2333\n",
       "[27](/login?next=%2Fpapers%2F2502.09560)\n",
       "\n",
       "### [EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied \n",
       "Agents](/papers/2502.09560)\n",
       "\n",
       "[* ![](/avatars/d9d8cfb5f112e6ed7f6152fc230135d3.svg)\n",
       "* ![](/avatars/02f1a9e2ce333ff521d901cf83fcdff3.svg)\n",
       "* ![](/avatars/81556de3214c848b3c3e118f50fd2968.svg)\n",
       "* ![](/avatars/c8c5b2706644fb45a75f13af99fa7ae9.svg)\n",
       "* ![](/avatars/c8b9ec7c00773c5a4055ba50de0c6b2f.svg)\n",
       "* ·\n",
       "  13 authors](/papers/2502.09560)\n",
       "[2](/papers/2502.09560#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09056.png)](/papers/2502.09056)\n",
       "Submitted by\n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg)\n",
       "akhaliq\n",
       "[26](/login?next=%2Fpapers%2F2502.09056)\n",
       "\n",
       "### [An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model \n",
       "Merging](/papers/2502.09056)\n",
       "\n",
       "[* \n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/63f6a050b4c9a104f4b95755/eJQyJkenSz536j-EGcpkH.jpeg)\n",
       "* ![](https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg)\n",
       "* ![](https://cdn-avatars.huggingface.co/v1/production/uploads/62d192c2d50433c35eb1b48e/VjmDu8GOIuLuQNBQdQLLS.png)\n",
       "* ·\n",
       "  4 authors](/papers/2502.09056)\n",
       "[4](/papers/2502.09056#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09604.png)](/papers/2502.09604)\n",
       "Submitted by\n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/1650651305661-5df84571da6d0311fd3d5407.png)\n",
       "voidism\n",
       "[26](/login?next=%2Fpapers%2F2502.09604)\n",
       "\n",
       "### [SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models](/papers/2502.09604)\n",
       "\n",
       "[* ![](/avatars/87708c86c1baef548ef556f5d32dca71.svg)\n",
       "* ![](https://cdn-ava\n",
       "..._This content has been truncated to stay below 10000 characters_...\n",
       "-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg)\n",
       "* ·\n",
       "  4 authors](/papers/2502.09042)\n",
       "[2](/papers/2502.09042#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09390.png)](/papers/2502.09390)\n",
       "Submitted by\n",
       "![](/avatars/4619930d15512ec9b80b01c62e986217.svg)\n",
       "danf\n",
       "[12](/login?next=%2Fpapers%2F2502.09390)\n",
       "\n",
       "### [SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language \n",
       "Models](/papers/2502.09390)\n",
       "\n",
       "[* ![](/avatars/e05ca715004b39e79472399f75010bda.svg)\n",
       "* ![](/avatars/b0022ad6dd992c75a51973070b302315.svg)\n",
       "* ![](https://cdn-avatars.huggingface.co/v1/production/uploads/63e0c8875c6964861ebb0c49/yzkhPSxgXtJCM62iMBOOK.jpeg)\n",
       "* ![](/avatars/4619930d15512ec9b80b01c62e986217.svg)\n",
       "* ·\n",
       "  4 authors](/papers/2502.09390)\n",
       "[2](/papers/2502.09390#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09601.png)](/papers/2502.09601)\n",
       "Submitted by\n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg)\n",
       "horseee\n",
       "[11](/login?next=%2Fpapers%2F2502.09601)\n",
       "\n",
       "### [CoT-Valve: Length-Compressible Chain-of-Thought Tuning](/papers/2502.09601)\n",
       "\n",
       "[* ![](/avatars/752e9d86018e7d33ad8bcd741203fd86.svg)\n",
       "* ![](/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg)\n",
       "* ![](/avatars/9216bdd9ed10c226f2d14edce4a10daa.svg)\n",
       "* ![](https://cdn-avatars.huggingface.co/v1/production/uploads/64396ebc21221ac7411852b3/SR0dC8N0bdj9tZFxYPpSf.jpeg)\n",
       "* ·\n",
       "  5 authors](/papers/2502.09601)\n",
       "[2](/papers/2502.09601#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08468.png)](/papers/2502.08468)\n",
       "Submitted by\n",
       "![](/avatars/b72b18130664c1de197c1f8df371aa70.svg)\n",
       "Haon-Chen\n",
       "[10](/login?next=%2Fpapers%2F2502.08468)\n",
       "\n",
       "### [mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data](/papers/2502.08468)\n",
       "\n",
       "[* ![](/avatars/be97941e60064e5dd806c6fe9db3c537.svg)\n",
       "* ![](/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg)\n",
       "* ![](/avatars/788e339472999a9159f77f857817d618.svg)\n",
       "* ![](https://cdn-avatars.huggingface.co/v1/production/uploads/625e62452a7279d3c77b5c38/zJINew6U4_Gup4WTobb-0.jpeg)\n",
       "* ![](/avatars/b72b18130664c1de197c1f8df371aa70.svg)\n",
       "* ·\n",
       "  7 authors](/papers/2502.08468)\n",
       "[2](/papers/2502.08468#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.08680.png)](/papers/2502.08680)\n",
       "Submitted by\n",
       "![](/avatars/6b67eba3f15d6cd86ac3ad55c1daf166.svg)\n",
       "guactastesgood\n",
       "[9](/login?next=%2Fpapers%2F2502.08680)\n",
       "\n",
       "### [Mathematical Reasoning in Large Language Models: Assessing Logical and Arithmetic Errors across Wide Numerical\n",
       "Ranges](/papers/2502.08680)\n",
       "\n",
       "[* ![](/avatars/6b67eba3f15d6cd86ac3ad55c1daf166.svg)\n",
       "* ![](/avatars/457eae5e56b9641ee5543146447d1755.svg)\n",
       "* ·\n",
       "  3 authors](/papers/2502.08680)\n",
       "[2](/papers/2502.08680#community)\n",
       "\n",
       "Submitted by\n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t4fI-3djMfgXCchU_xpjL.png)\n",
       "xymeow7\n",
       "[9](/login?next=%2Fpapers%2F2502.09614)\n",
       "\n",
       "### [DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human \n",
       "References](/papers/2502.09614)\n",
       "\n",
       "[* ![](https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t4fI-3djMfgXCchU_xpjL.png)\n",
       "* ·\n",
       "  5 authors](/papers/2502.09614)\n",
       "[2](/papers/2502.09614#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05761.png)](/papers/2502.05761)\n",
       "Submitted by\n",
       "![](/avatars/a4b7b7fd6c1fca0eac85da7383f58361.svg)\n",
       "enquan2022\n",
       "[6](/login?next=%2Fpapers%2F2502.05761)\n",
       "\n",
       "### [3CAD: A Large-Scale Real-World 3C Product Dataset for Unsupervised Anomaly](/papers/2502.05761)\n",
       "\n",
       "[* ![](/avatars/a4b7b7fd6c1fca0eac85da7383f58361.svg)\n",
       "* ·\n",
       "  7 authors](/papers/2502.05761)\n",
       "[2](/papers/2502.05761#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.09613.png)](/papers/2502.09613)\n",
       "Submitted by\n",
       "![](/avatars/172678e8187acfec0aa8e647479cbb81.svg)\n",
       "chaoyizh\n",
       "[5](/login?next=%2Fpapers%2F2502.09613)\n",
       "\n",
       "### [Latent Radiance Fields with 3D-aware 2D Representations](/papers/2502.09613)\n",
       "\n",
       "[* ·\n",
       "  4 authors](/papers/2502.09613)\n",
       "[2](/papers/2502.09613#community)\n",
       "\n",
       "[![](https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2502.05979.png)](/papers/2502.05979)\n",
       "Submitted by\n",
       "![](https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg)\n",
       "BestWishYsh\n",
       "[4](/login?next=%2Fpapers%2F2502.05979)\n",
       "\n",
       "### [VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer](/papers/2502.05979)\n",
       "\n",
       "[* ![](/avatars/6cb41b80cc5e014e455dfc2a22682e64.svg)\n",
       "* ![](/avatars/06a3f3cd533caa459f3da1bb5e0f4c1f.svg)\n",
       "* ·\n",
       "  7 authors](/papers/2502.05979)\n",
       "[2](/papers/2502.05979#community)\n",
       "\n",
       "[Previous](/papers?date=2025-02-13)\n",
       "\n",
       "System theme\n",
       "\n",
       "Company\n",
       "[TOS](/terms-of-service)\n",
       "[Privacy](/privacy)\n",
       "[About](/huggingface)\n",
       "[Jobs](https://apply.workable.com/huggingface/)\n",
       "\n",
       "Website\n",
       "[Models](/models)\n",
       "[Datasets](/datasets)\n",
       "[Spaces](/spaces)\n",
       "[Pricing](/pricing)\n",
       "[Docs](/docs)\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 7.66 seconds| Input tokens: 8,930 | Output tokens: 320]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 7.66 seconds| Input tokens: 8,930 | Output tokens: 320]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m4\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_title </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept </span><span style=\"background-color: #272822\">            </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Understanding\"</span><span style=\"background-color: #272822\">                                                                                                 </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_id </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> get_paper_id_by_title(title</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_title)</span><span style=\"background-color: #272822\">                                                            </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Paper ID:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, paper_id)</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpaper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mThe Stochastic Parrot on LLM\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34ms Shoulder: A Summative Assessment of Physical Concept \u001b[0m\u001b[48;2;39;40;34m            \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mUnderstanding\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                                                                                 \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mget_paper_id_by_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtitle\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_title\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                            \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mPaper ID:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Paper ID: 2502.08946\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Paper ID: 2502.08946\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 14.92 seconds| Input tokens: 17,847 | Output tokens: 467]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 14.92 seconds| Input tokens: 17,847 | Output tokens: 467]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m5\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">download_paper_by_id(paper_id</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"2502.08946\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">paper_content </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> read_pdf_file(file_path</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"paper.pdf\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                           </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(paper_content)</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mdownload_paper_by_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_id\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m2502.08946\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpaper_content\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mread_pdf_file\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfile_path\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mpaper.pdf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                           \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpaper_content\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "The Stochastic Parrot on LLM’s Shoulder:\n",
       "A Summative Assessment of Physical Concept Understanding\n",
       "Mo Yu1* Lemao Liu1* Junjie Wu2* Tsz Ting Chung2* Shunchi Zhang3*\n",
       "Jiangnan Li1 Dit-Yan Yeung2 Jie Zhou1\n",
       "1WeChat AI, Tencent 2HKUST 3JHU\n",
       "moyumyu@global.tencent.com redmondliu@tencent.com\n",
       "{junjie.wu,ttchungac}@connect.ust.hk szhan256@cs.jhu.edu\n",
       "https://physico-benchmark.github.io\n",
       "Abstract\n",
       "In a systematic way, we investigate a widely\n",
       "asked question: Do LLMs really understand\n",
       "what they say?, which relates to the more famil-\n",
       "iar term Stochastic Parrot. To this end, we pro-\n",
       "pose a summative assessment over a carefully\n",
       "designed physical concept understanding task,\n",
       "PHYSI CO. Our task alleviates the memoriza-\n",
       "tion issue via the usage of grid-format inputs\n",
       "that abstractly describe physical phenomena.\n",
       "The grids represents varying levels of under-\n",
       "standing, from the core phenomenon, applica-\n",
       "tion examples to analogies to other abstract\n",
       "patterns in the grid world. A comprehensive\n",
       "study on our task demonstrates: (1) state-of-\n",
       "the-art LLMs, including GPT-4o, o1 and Gem-\n",
       "ini 2.0 flash thinking, lag behind humans by\n",
       "∼40%; (2) the stochastic parrot phenomenon\n",
       "is present in LLMs, as they fail on our grid\n",
       "task but can describe and recognize the same\n",
       "concepts well in natural language; (3) our task\n",
       "challenges the LLMs due to intrinsic difficul-\n",
       "ties rather than the unfamiliar grid format, as\n",
       "in-context learning and fine-tuning on same for-\n",
       "matted data added little to their performance.\n",
       "1 Introduction\n",
       "Recent years have witnessed remarkable advance-\n",
       "ments in large language models (LLMs) (Brown\n",
       "et al., 2020; Achiam et al., 2023; Team et al., 2023).\n",
       "Thanks to the substantial model capacity and mas-\n",
       "sive training data, LLMs have achieved new state-\n",
       "of-the-arts on a variety of NLP tasks, even surpass-\n",
       "ing humans on some of them (Min et al., 2023;\n",
       "Chang et al., 2024). Nowadays the application of\n",
       "LLMs has become widespread, facilitating daily\n",
       "work and life, and profoundly influencing people’s\n",
       "work and lifestyles (Bommasani et al., 2021; Peng\n",
       "et al., 2024; Demszky et al., 2023).\n",
       "On the other hand, despite the great success of\n",
       "LLMs, many researchers argue that LLMs may not\n",
       "*Equal contribution.\n",
       "Figure 1: Illustration of a “Stochastic Parrot” by our\n",
       "PHYSI CO task consisting of both low-level and high-\n",
       "level subtasks in parallel. For a concept Gravity, an\n",
       "LLM can generate its accurate description in natural lan-\n",
       "guage, but cannot interpret its grid-format illustration.\n",
       "really understand what they claim they do (Ben-\n",
       "der and Koller, 2020; Bender et al., 2021; Bom-\n",
       "masani et al., 2021; Mitchell and Krakauer, 2023)\n",
       "due to their strong memorization ability. In par-\n",
       "ticular, Bender et al. (2021) questioned whether\n",
       "LLMs are just Stochastic Parrotsthat repeat words\n",
       "based on correlations without true understanding.\n",
       "This argument has been acknowledged by many\n",
       "research papers and dozens of them even include\n",
       "this term in their titles.1 Unfortunately, to our best\n",
       "knowledge, there are no quantitative experiments to\n",
       "verify the stochastic parrot phenomenon in LLMs.\n",
       "Existing studies indicate that LLMs may fail on\n",
       "one particular challenging task (Chakrabarty et al.,\n",
       "2022; Shapira et al., 2023; Hessel et al., 2023; Tong\n",
       "et al., 2024), but they do not demonstrate that LLMs\n",
       "claimed to understand those tasks by providing a\n",
       "controlled and paired evidence.\n",
       "This paper aims to provide quantitative evidence\n",
       "to validate the argument of stochastic parrot in\n",
       "LLMs. To this end, from the perspective of ed-\n",
       "ucational and cognitive psychology, we first em-\n",
       "ploy the approach of summative assessment (Black\n",
       "and Wiliam, 1998a,b) to measure understanding in\n",
       "LLMs. Its key idea is to design various tasks that\n",
       "1https://scholar.google.com/scholar?hl=en&amp;q=\n",
       "llms+are+stochastic+parrot.\n",
       "1\n",
       "arXiv:2502.08946v1  [cs.CL]  13 Feb 2025test different understanding levels regarding a spe-\n",
       "cific concept. Following the principle of Bloom’s\n",
       "taxonomy (Armstrong, 2010; Krathwohl, 2002),\n",
       "we design tasks that reflect different levels of un-\n",
       "derstanding. Consequently, we develop PHYSI CO,\n",
       "a task designed to assess understanding of basic\n",
       "physical concepts from high school such asGravity.\n",
       "Our focus on physical concepts stems from both\n",
       "their fundamental relevance to important topics of\n",
       "world models and embodied systems (Savva et al.,\n",
       "2019; Duan et al., 2022; Xiang et al., 2023), and\n",
       "their rich denotations and connotations that enable\n",
       "effective design of summative assessment tasks.\n",
       "Specifically, PHYSI CO includes two subtasks\n",
       "corresponding to two coarse levels of understand-\n",
       "ing in Bloom’s taxonomy, as shown in Figure 1.\n",
       "One is the low-level understanding subtask in the\n",
       "natural language format, aimed at measuring the\n",
       "remembering (or memorization) ability of LLMs.\n",
       "The other involves the same concepts but in an ab-\n",
       "stract representation format inspired by (Chollet,\n",
       "2019), which is designed to measure the high-level\n",
       "understanding beyond remembering of LLMs.\n",
       "We conduct comprehensive experiments on\n",
       "PHYSI CO with representative open-source and\n",
       "commercial LLMs.2 We obtain two key findings:\n",
       "(1) State-of-the-art LLMs perform perfectly on the\n",
       "low-level understanding subtask (&gt;95% in Accu-\n",
       "racy) but lags behind humans by a large margin\n",
       "(∼40% in Accuracy) on the high-level subtask,\n",
       "which verifies the stochastic parrot phenomenon in\n",
       "LLMs. (2) Further analysis shows that our high-\n",
       "level subtask challenges LLMs due to the intrinsic\n",
       "difficulty of deep understanding rather than the un-\n",
       "familiar format.\n",
       "This paper makes the following contributions:\n",
       "• We introduce a psychology-appealing approach\n",
       "(summative assessment) and a corresponding\n",
       "task PHYSI CO to measure the understanding of\n",
       "LLMs.\n",
       "• Based on PHYSI CO, we provide a quantitative\n",
       "experiment to successfully verify the stochastic\n",
       "parrot phenomenon in LLMs.\n",
       "• As a by-product, our work presents a challeng-\n",
       "ing comprehension task for existing text-only\n",
       "and multimodal LLMs, which establishes a sub-\n",
       "stantial performance gap between humans and\n",
       "machines.\n",
       "2Throughout this paper, LLM refers to either standard text-\n",
       "only LLMs or large multimodal models for simplicity.\n",
       "2 Measuring Concept Understanding via\n",
       "Summative Assessment\n",
       "It is intrinsically challenging to measure the extent\n",
       "to which LLMs understand a sentence or concept.\n",
       "Indeed, Bender and Koller (2020) provide a defini-\n",
       "tion of \"understanding\" from a linguistic perspec-\n",
       "tive, but this definition depends on another abstract\n",
       "and unmeasurable term, “ meaning”. Therefore,\n",
       "even with this definition, accurately measuring \"un-\n",
       "derstanding\" remains elusive.\n",
       "We approach the measurement of whether LLMs\n",
       "understand a concept from an educational and\n",
       "cognitive perspective, using summative assess-\n",
       "ment (Black and Wiliam, 1998a,b; Harlen and\n",
       "James, 1997). Summative assessment is widely\n",
       "used by educators as an appealing strategy to eval-\n",
       "uate students’ understanding and knowledge ac-\n",
       "quisition in educational and cognitive psychology.\n",
       "For example, when middle school physics teachers\n",
       "want to know whether a student truly understands\n",
       "the concept “Gravity”, they would design a series\n",
       "of questions specifically related to the concept of\n",
       "gravity to assess comprehension, e.g., the prop-\n",
       "erties like inverse square law and examples like\n",
       "orbital motions. If a student struggles to answer\n",
       "many of these questions, the teacher may conclude\n",
       "that the student does not understand the concept\n",
       "well or has a poor grasp of it.\n",
       "We extend the idea of summative assessment to\n",
       "evaluating the concept understanding of machines.\n",
       "Formally, assume S denotes an intelligent system\n",
       "and C is a specific concept. To evaluate the extent\n",
       "how S understands the concept C, our summative\n",
       "assessment includes the following two steps:\n",
       "• Task design towards C: design several concept\n",
       "understanding tasks, each of which consists of\n",
       "several questions manually created towards un-\n",
       "derstanding the concept C.\n",
       "• Evaluating S: ask S to answer the questions from\n",
       "the tasks and calculate its accuracy.\n",
       "Requirements for Validity The success (valid-\n",
       "ity) of the proposed evaluation approach highly\n",
       "depends on the task design (Black and Wiliam,\n",
       "1998a,b). For example, if the questions are too easy,\n",
       "even a weak system could answer them correctly.\n",
       "This leads to an overestimation of the system’s un-\n",
       "derstanding capabilities, making the assessment in-\n",
       "effective. To ensure good validity, we adhere to the\n",
       "principles outlined in summative assessment (Black\n",
       "and Wiliam, 1998a,b) for task design:\n",
       "2• Alignment with evaluating objectives: the ques-\n",
       "tions should be related to the targeted concept,\n",
       "and should measure the specific knowledge about\n",
       "the targeted concept.\n",
       "• Different difficulty levels: the questions should\n",
       "be with different difficulty levels from easy to\n",
       "difficult level, to ensure that the evaluation results\n",
       "have distinctiveness for different systems.\n",
       "• Variety: the questions should reflect various un-\n",
       "derstanding aspects of the targeted concept; ad-\n",
       "dressing both its denotation and connotation.\n",
       "• Simplicity: while not mandatory, a simpler bench-\n",
       "mark for humans can more effectively high-\n",
       "light the issue faced by current models, i.e., the\n",
       "stochastic parrot effect in LLMs.\n",
       "3 Task Design and Dataset Construction\n",
       "3.1 Task Design Principle\n",
       "We borrow the idea of Bloom’s taxonomy (Krath-\n",
       "wohl, 2002; Armstrong, 2010) from education re-\n",
       "search to fulfill the requirements for task design\n",
       "in Section 2, so as to ensure the assessment valid-\n",
       "ity. Bloom’s taxonomy offers an ideal principle to\n",
       "these requirements with an ordering of six cogni-\n",
       "tive skills (from low to high level) for knowledge\n",
       "understanding: Remembering, Understanding, Ap-\n",
       "plying, Analyzing, Evaluating and Creating.\n",
       "Generally, it is nontrivial to strictly follow this\n",
       "principle since there is no clear boundary among\n",
       "the last four skills of understanding. As a result,\n",
       "we group the last four high-level skills into one and\n",
       "consider the following two levels of understanding:\n",
       "• Low-level Understanding : covering the two\n",
       "lowest-level skills in Bloom’s taxonomy, i.e.,\n",
       "retrieving relevant knowledge from long-term\n",
       "memory and rephrasing in one’s own words.\n",
       "• High-level Understanding: covering the aspects\n",
       "for understanding the knowledge beyond mem-\n",
       "orization. As shown by the examples in Section\n",
       "3.2.2, our tasks directly correspond to a spec-\n",
       "trum from the understanding level of applying to\n",
       "the level of analyzing in Bloom’s taxonomy,e.g.,\n",
       "applying the knowledge to explain a physical\n",
       "phenomenon, analyzing a concrete property of a\n",
       "concept in a generalized and abstract manner,3.\n",
       "Based on these two levels, we design the following\n",
       "PHYSI CO task for summative assessment.\n",
       "3For example, the flow of electric current can be abstracted\n",
       "as moving from high potential to low potential.\n",
       "3.2 Our P HYSI CO Task\n",
       "PHYSI CO is essentially a physical concept under-\n",
       "standing task, which primarily targets on 52 phys-\n",
       "ical concepts or phenomena: e.g., gravity, light\n",
       "reflection, acceleration, buoyancy, inertia, etc (see\n",
       "Appendix A for the full list). Our focus on physi-\n",
       "cal concepts is motivated by two main reasons: 1)\n",
       "understanding physical concepts is critical for intel-\n",
       "ligent systems to interact with the world, which is\n",
       "ultimate goal of embodied AI (Savva et al., 2019;\n",
       "Duan et al., 2022; Xiang et al., 2023); 2) design-\n",
       "ing tasks centered around physical concepts allows\n",
       "us to more easily control different levels of under-\n",
       "standing and ensure the diversity of each concept.\n",
       "For each physical concept, PHYSI CO involves\n",
       "both low-level understanding subtasks and high-\n",
       "level subtasks, following our task design principles.\n",
       "3.2.1 Low-level Understanding Subtasks\n",
       "Physical Concept Selection (text) First, to eval-\n",
       "uate whether an LLM possesses the knowledge of\n",
       "our included concepts, we design a task to recog-\n",
       "nize a concept from its corresponding Wikipedia\n",
       "definition. Specifically, we manually masked\n",
       "the synonyms of the concept with placeholder\n",
       "[PHENOMENON]. Meanwhile, highly relevant enti-\n",
       "ties were masked as [MASK] to alleviate shortcuts.\n",
       "For example, in the definition of Gravity, the\n",
       "terms “gravity” and “gravitation” were masked as\n",
       "[PHENOMENON], while “Isaac Newton” was masked as\n",
       "[MASK]. Details can be found in Appendix B. We\n",
       "then present the LLMs with the same four choices\n",
       "as in our following high-level subtasks.\n",
       "Physical Concept Selection (visual) Second, we\n",
       "evaluate if the LLMs can recognize our concepts\n",
       "represented with real-life pictures. To this end,\n",
       "we query our concepts on Google image search,\n",
       "and select the images that reflect the same core\n",
       "properties and examples annotated in our following\n",
       "high-level tasks. This results in 100 examples. We\n",
       "construct the same four-choice instances as above.\n",
       "Physical Concept Generation Finally, we di-\n",
       "rectly ask the LLMs to generate the description of\n",
       "a concept with its core properties and representa-\n",
       "tive examples. For instance, the concept Gravity is\n",
       "described as “a force that pulls objects with mass\n",
       "towards each other”, followed by the example “an\n",
       "apple falls to the ground” as shown in Figure 1. We\n",
       "then evaluate the performance of LLMs by measur-\n",
       "ing the quality of the description and its coverage\n",
       "of knowledge required by our PHYSI CO and we\n",
       "3\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "The Stochastic Parrot on LLM’s Shoulder:\n",
       "A Summative Assessment of Physical Concept Understanding\n",
       "Mo Yu1* Lemao Liu1* Junjie Wu2* Tsz Ting Chung2* Shunchi Zhang3*\n",
       "Jiangnan Li1 Dit-Yan Yeung2 Jie Zhou1\n",
       "1WeChat AI, Tencent 2HKUST 3JHU\n",
       "moyumyu@global.tencent.com redmondliu@tencent.com\n",
       "{junjie.wu,ttchungac}@connect.ust.hk szhan256@cs.jhu.edu\n",
       "https://physico-benchmark.github.io\n",
       "Abstract\n",
       "In a systematic way, we investigate a widely\n",
       "asked question: Do LLMs really understand\n",
       "what they say?, which relates to the more famil-\n",
       "iar term Stochastic Parrot. To this end, we pro-\n",
       "pose a summative assessment over a carefully\n",
       "designed physical concept understanding task,\n",
       "PHYSI CO. Our task alleviates the memoriza-\n",
       "tion issue via the usage of grid-format inputs\n",
       "that abstractly describe physical phenomena.\n",
       "The grids represents varying levels of under-\n",
       "standing, from the core phenomenon, applica-\n",
       "tion examples to analogies to other abstract\n",
       "patterns in the grid world. A comprehensive\n",
       "study on our task demonstrates: (1) state-of-\n",
       "the-art LLMs, including GPT-4o, o1 and Gem-\n",
       "ini 2.0 flash thinking, lag behind humans by\n",
       "∼40%; (2) the stochastic parrot phenomenon\n",
       "is present in LLMs, as they fail on our grid\n",
       "task but can describe and recognize the same\n",
       "concepts well in natural language; (3) our task\n",
       "challenges the LLMs due to intrinsic difficul-\n",
       "ties rather than the unfamiliar grid format, as\n",
       "in-context learning and fine-tuning on same for-\n",
       "matted data added little to their performance.\n",
       "1 Introduction\n",
       "Recent years have witnessed remarkable advance-\n",
       "ments in large language models (LLMs) (Brown\n",
       "et al., 2020; Achiam et al., 2023; Team et al., 2023).\n",
       "Thanks to the substantial model capacity and mas-\n",
       "sive training data, LLMs have achieved new state-\n",
       "of-the-arts on a variety of NLP tasks, even surpass-\n",
       "ing humans on some of them (Min et al., 2023;\n",
       "Chang et al., 2024). Nowadays the application of\n",
       "LLMs has become widespread, facilitating daily\n",
       "work and life, and profoundly influencing people’s\n",
       "work and lifestyles (Bommasani et al., 2021; Peng\n",
       "et al., 2024; Demszky et al., 2023).\n",
       "On the other hand, despite the great success of\n",
       "LLMs, many researchers argue that LLMs may not\n",
       "*Equal contribution.\n",
       "Figure 1: Illustration of a “Stochastic Parrot” by our\n",
       "PHYSI CO task consisting of both low-level and high-\n",
       "level subtasks in parallel. For a concept Gravity, an\n",
       "LLM can generate its accurate description in natural lan-\n",
       "guage, but cannot interpret its grid-format illustration.\n",
       "really understand what they claim they do (Ben-\n",
       "der and Koller, 2020; Bender et al., 2021; Bom-\n",
       "masani et al., 2021; Mitchell and Krakauer, 2023)\n",
       "due to their strong memorization ability. In par-\n",
       "ticular, Bender et al. (2021) questioned whether\n",
       "LLMs are just Stochastic Parrotsthat repeat words\n",
       "based on correlations without true understanding.\n",
       "This argument has been acknowledged by many\n",
       "research papers and dozens of them even include\n",
       "this term in their titles.1 Unfortunately, to our best\n",
       "knowledge, there are no quantitative experiments to\n",
       "verify the stochastic parrot phenomenon in LLMs.\n",
       "Existing studies indicate that LLMs may fail on\n",
       "one particular challenging task (Chakrabarty et al.,\n",
       "2022; Shapira et al., 2023; Hessel et al., 2023; Tong\n",
       "et al., 2024), but they do not demonstrate that LLMs\n",
       "claimed to understand those tasks by providing a\n",
       "controlled and paired evidence.\n",
       "This paper aims to provide quantitative evidence\n",
       "to validate the argument of stochastic parrot in\n",
       "LLMs. To this end, from the perspective of ed-\n",
       "ucational and cognitive psychology, we first em-\n",
       "ploy the approach of summative assessment (Black\n",
       "and Wiliam, 1998a,b) to measure understanding in\n",
       "LLMs. Its key idea is to design various tasks that\n",
       "1https://scholar.google.com/scholar?hl=en&q=\n",
       "llms+are+stochastic+parrot.\n",
       "1\n",
       "arXiv:2502.08946v1  [cs.CL]  13 Feb 2025test different understanding levels regarding a spe-\n",
       "cific concept. Following the principle of Bloom’s\n",
       "taxonomy (Armstrong, 2010; Krathwohl, 2002),\n",
       "we design tasks that reflect different levels of un-\n",
       "derstanding. Consequently, we develop PHYSI CO,\n",
       "a task designed to assess understanding of basic\n",
       "physical concepts from high school such asGravity.\n",
       "Our focus on physical concepts stems from both\n",
       "their fundamental relevance to important topics of\n",
       "world models and embodied systems (Savva et al.,\n",
       "2019; Duan et al., 2022; Xiang et al., 2023), and\n",
       "their rich denotations and connotations that enable\n",
       "effective design of summative assessment tasks.\n",
       "Specifically, PHYSI CO includes two subtasks\n",
       "corresponding to two coarse levels of understand-\n",
       "ing in Bloom’s taxonomy, as shown in Figure 1.\n",
       "One is the low-level understanding subtask in the\n",
       "natural language format, aimed at measuring the\n",
       "remembering (or memorization) ability of LLMs.\n",
       "The other involves the same concepts but in an ab-\n",
       "stract representation format inspired by (Chollet,\n",
       "2019), which is designed to measure the high-level\n",
       "understanding beyond remembering of LLMs.\n",
       "We conduct comprehensive experiments on\n",
       "PHYSI CO with representative open-source and\n",
       "commercial LLMs.2 We obtain two key findings:\n",
       "(1) State-of-the-art LLMs perform perfectly on the\n",
       "low-level understanding subtask (>95% in Accu-\n",
       "racy) but lags behind humans by a large margin\n",
       "(∼40% in Accuracy) on the high-level subtask,\n",
       "which verifies the stochastic parrot phenomenon in\n",
       "LLMs. (2) Further analysis shows that our high-\n",
       "level subtask challenges LLMs due to the intrinsic\n",
       "difficulty of deep understanding rather than the un-\n",
       "familiar format.\n",
       "This paper makes the following contributions:\n",
       "• We introduce a psychology-appealing approach\n",
       "(summative assessment) and a corresponding\n",
       "task PHYSI CO to measure the understanding of\n",
       "LLMs.\n",
       "• Based on PHYSI CO, we provide a quantitative\n",
       "experiment to successfully verify the stochastic\n",
       "parrot phenomenon in LLMs.\n",
       "• As a by-product, our work presents a challeng-\n",
       "ing comprehension task for existing text-only\n",
       "and multimodal LLMs, which establishes a sub-\n",
       "stantial performance gap between humans and\n",
       "machines.\n",
       "2Throughout this paper, LLM refers to either standard text-\n",
       "only LLMs or large multimodal models for simplicity.\n",
       "2 Measuring Concept Understanding via\n",
       "Summative Assessment\n",
       "It is intrinsically challenging to measure the extent\n",
       "to which LLMs understand a sentence or concept.\n",
       "Indeed, Bender and Koller (2020) provide a defini-\n",
       "tion of \"understanding\" from a linguistic perspec-\n",
       "tive, but this definition depends on another abstract\n",
       "and unmeasurable term, “ meaning”. Therefore,\n",
       "even with this definition, accurately measuring \"un-\n",
       "derstanding\" remains elusive.\n",
       "We approach the measurement of whether LLMs\n",
       "understand a concept from an educational and\n",
       "cognitive perspective, using summative assess-\n",
       "ment (Black and Wiliam, 1998a,b; Harlen and\n",
       "James, 1997). Summative assessment is widely\n",
       "used by educators as an appealing strategy to eval-\n",
       "uate students’ understanding and knowledge ac-\n",
       "quisition in educational and cognitive psychology.\n",
       "For example, when middle school physics teachers\n",
       "want to know whether a student truly understands\n",
       "the concept “Gravity”, they would design a series\n",
       "of questions specifically related to the concept of\n",
       "gravity to assess comprehension, e.g., the prop-\n",
       "erties like inverse square law and examples like\n",
       "orbital motions. If a student struggles to answer\n",
       "many of these questions, the teacher may conclude\n",
       "that the student does not understand the concept\n",
       "well or has a poor grasp of it.\n",
       "We extend the idea of summative assessment to\n",
       "evaluating the concept understanding of machines.\n",
       "Formally, assume S denotes an intelligent system\n",
       "and C is a specific concept. To evaluate the extent\n",
       "how S understands the concept C, our summative\n",
       "assessment includes the following two steps:\n",
       "• Task design towards C: design several concept\n",
       "understanding tasks, each of which consists of\n",
       "several questions manually created towards un-\n",
       "derstanding the concept C.\n",
       "• Evaluating S: ask S to answer the questions from\n",
       "the tasks and calculate its accuracy.\n",
       "Requirements for Validity The success (valid-\n",
       "ity) of the proposed evaluation approach highly\n",
       "depends on the task design (Black and Wiliam,\n",
       "1998a,b). For example, if the questions are too easy,\n",
       "even a weak system could answer them correctly.\n",
       "This leads to an overestimation of the system’s un-\n",
       "derstanding capabilities, making the assessment in-\n",
       "effective. To ensure good validity, we adhere to the\n",
       "principles outlined in summative assessment (Black\n",
       "and Wiliam, 1998a,b) for task design:\n",
       "2• Alignment with evaluating objectives: the ques-\n",
       "tions should be related to the targeted concept,\n",
       "and should measure the specific knowledge about\n",
       "the targeted concept.\n",
       "• Different difficulty levels: the questions should\n",
       "be with different difficulty levels from easy to\n",
       "difficult level, to ensure that the evaluation results\n",
       "have distinctiveness for different systems.\n",
       "• Variety: the questions should reflect various un-\n",
       "derstanding aspects of the targeted concept; ad-\n",
       "dressing both its denotation and connotation.\n",
       "• Simplicity: while not mandatory, a simpler bench-\n",
       "mark for humans can more effectively high-\n",
       "light the issue faced by current models, i.e., the\n",
       "stochastic parrot effect in LLMs.\n",
       "3 Task Design and Dataset Construction\n",
       "3.1 Task Design Principle\n",
       "We borrow the idea of Bloom’s taxonomy (Krath-\n",
       "wohl, 2002; Armstrong, 2010) from education re-\n",
       "search to fulfill the requirements for task design\n",
       "in Section 2, so as to ensure the assessment valid-\n",
       "ity. Bloom’s taxonomy offers an ideal principle to\n",
       "these requirements with an ordering of six cogni-\n",
       "tive skills (from low to high level) for knowledge\n",
       "understanding: Remembering, Understanding, Ap-\n",
       "plying, Analyzing, Evaluating and Creating.\n",
       "Generally, it is nontrivial to strictly follow this\n",
       "principle since there is no clear boundary among\n",
       "the last four skills of understanding. As a result,\n",
       "we group the last four high-level skills into one and\n",
       "consider the following two levels of understanding:\n",
       "• Low-level Understanding : covering the two\n",
       "lowest-level skills in Bloom’s taxonomy, i.e.,\n",
       "retrieving relevant knowledge from long-term\n",
       "memory and rephrasing in one’s own words.\n",
       "• High-level Understanding: covering the aspects\n",
       "for understanding the knowledge beyond mem-\n",
       "orization. As shown by the examples in Section\n",
       "3.2.2, our tasks directly correspond to a spec-\n",
       "trum from the understanding level of applying to\n",
       "the level of analyzing in Bloom’s taxonomy,e.g.,\n",
       "applying the knowledge to explain a physical\n",
       "phenomenon, analyzing a concrete property of a\n",
       "concept in a generalized and abstract manner,3.\n",
       "Based on these two levels, we design the following\n",
       "PHYSI CO task for summative assessment.\n",
       "3For example, the flow of electric current can be abstracted\n",
       "as moving from high potential to low potential.\n",
       "3.2 Our P HYSI CO Task\n",
       "PHYSI CO is essentially a physical concept under-\n",
       "standing task, which primarily targets on 52 phys-\n",
       "ical concepts or phenomena: e.g., gravity, light\n",
       "reflection, acceleration, buoyancy, inertia, etc (see\n",
       "Appendix A for the full list). Our focus on physi-\n",
       "cal concepts is motivated by two main reasons: 1)\n",
       "understanding physical concepts is critical for intel-\n",
       "ligent systems to interact with the world, which is\n",
       "ultimate goal of embodied AI (Savva et al., 2019;\n",
       "Duan et al., 2022; Xiang et al., 2023); 2) design-\n",
       "ing tasks centered around physical concepts allows\n",
       "us to more easily control different levels of under-\n",
       "standing and ensure the diversity of each concept.\n",
       "For each physical concept, PHYSI CO involves\n",
       "both low-level understanding subtasks and high-\n",
       "level subtasks, following our task design principles.\n",
       "3.2.1 Low-level Understanding Subtasks\n",
       "Physical Concept Selection (text) First, to eval-\n",
       "uate whether an LLM possesses the knowledge of\n",
       "our included concepts, we design a task to recog-\n",
       "nize a concept from its corresponding Wikipedia\n",
       "definition. Specifically, we manually masked\n",
       "the synonyms of the concept with placeholder\n",
       "[PHENOMENON]. Meanwhile, highly relevant enti-\n",
       "ties were masked as [MASK] to alleviate shortcuts.\n",
       "For example, in the definition of Gravity, the\n",
       "terms “gravity” and “gravitation” were masked as\n",
       "[PHENOMENON], while “Isaac Newton” was masked as\n",
       "[MASK]. Details can be found in Appendix B. We\n",
       "then present the LLMs with the same four choices\n",
       "as in our following high-level subtasks.\n",
       "Physical Concept Selection (visual) Second, we\n",
       "evaluate if the LLMs can recognize our concepts\n",
       "represented with real-life pictures. To this end,\n",
       "we query our concepts on Google image search,\n",
       "and select the images that reflect the same core\n",
       "properties and examples annotated in our following\n",
       "high-level tasks. This results in 100 examples. We\n",
       "construct the same four-choice instances as above.\n",
       "Physical Concept Generation Finally, we di-\n",
       "rectly ask the LLMs to generate the description of\n",
       "a concept with its core properties and representa-\n",
       "tive examples. For instance, the concept Gravity is\n",
       "described as “a force that pulls objects with mass\n",
       "towards each other”, followed by the example “an\n",
       "apple falls to the ground” as shown in Figure 1. We\n",
       "then evaluate the performance of LLMs by measur-\n",
       "ing the quality of the description and its coverage\n",
       "of knowledge required by our PHYSI CO and we\n",
       "3\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 6.59 seconds| Input tokens: 27,040 | Output tokens: 566]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 4: Duration 6.59 seconds| Input tokens: 27,040 | Output tokens: 566]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m6\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"The paper 'The Stochastic Parrot on LLM’s Shoulder: A Summative Assessment of Physical Concept </span><span style=\"background-color: #272822\">  </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">Understanding' investigates whether LLMs truly understand their outputs. By introducing the PHYSI CO task, the </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">authors assess different levels of understanding using Bloom’s taxonomy. They find that while state-of-the-art </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">LLMs perform well on low-level tasks, they struggle with high-level tasks, demonstrating the Stochastic Parrot </span>  \n",
       "  <span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">phenomenon.\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                                  </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mThe paper \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mThe Stochastic Parrot on LLM’s Shoulder: A Summative Assessment of Physical Concept \u001b[0m\u001b[48;2;39;40;34m  \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mUnderstanding\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m investigates whether LLMs truly understand their outputs. By introducing the PHYSI CO task, the \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mauthors assess different levels of understanding using Bloom’s taxonomy. They find that while state-of-the-art \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mLLMs perform well on low-level tasks, they struggle with high-level tasks, demonstrating the Stochastic Parrot \u001b[0m  \n",
       "  \u001b[38;2;230;219;116;48;2;39;40;34mphenomenon.\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                  \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Out - Final answer: The paper 'The Stochastic Parrot on LLM’s Shoulder: A Summative Assessment of Physical Concept </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Understanding' investigates whether LLMs truly understand their outputs. By introducing the PHYSI CO task, the </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">authors assess different levels of understanding using Bloom’s taxonomy. They find that while state-of-the-art LLMs</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">perform well on low-level tasks, they struggle with high-level tasks, demonstrating the Stochastic Parrot </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">phenomenon.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mOut - Final answer: The paper 'The Stochastic Parrot on LLM’s Shoulder: A Summative Assessment of Physical Concept \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mUnderstanding' investigates whether LLMs truly understand their outputs. By introducing the PHYSI CO task, the \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mauthors assess different levels of understanding using Bloom’s taxonomy. They find that while state-of-the-art LLMs\u001b[0m\n",
       "\u001b[1;38;2;212;183;2mperform well on low-level tasks, they struggle with high-level tasks, demonstrating the Stochastic Parrot \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mphenomenon.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 5: Duration 17.41 seconds| Input tokens: 39,602 | Output tokens: 840]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 5: Duration 17.41 seconds| Input tokens: 39,602 | Output tokens: 840]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"The paper 'The Stochastic Parrot on LLM’s Shoulder: A Summative Assessment of Physical Concept Understanding' investigates whether LLMs truly understand their outputs. By introducing the PHYSI CO task, the authors assess different levels of understanding using Bloom’s taxonomy. They find that while state-of-the-art LLMs perform well on low-level tasks, they struggle with high-level tasks, demonstrating the Stochastic Parrot phenomenon.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from smolagents import CodeAgent, HfApiModel\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
    "\n",
    "#model = HfApiModel(model_id=model_id, token=api_key)\n",
    "model = LiteLLMModel(model_id=\"claude-3-5-sonnet-20240620\",\n",
    "                     api_key=anthropic_api_key)\n",
    "\n",
    "agent = CodeAgent(tools=[get_hugging_face_top_daily_paper,\n",
    "                         get_paper_id_by_title,\n",
    "                         download_paper_by_id,\n",
    "                         read_pdf_file],\n",
    "                  model=model,\n",
    "                  add_base_tools=True)\n",
    "\n",
    "agent.run(\n",
    "    \"Summarize today's top paper on Hugging Face daily papers by reading it.\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
